2022-04-26 23:06:49,679 BERTClassifier INFO Apply model bert
2022-04-26 23:06:52,554 datasets.builder WARNING Using custom data configuration default-b47e4a4aa98e97e4
2022-04-26 23:06:52,554 datasets.builder WARNING Reusing dataset csv (C:\Users\Phuong\.cache\huggingface\datasets\csv\default-b47e4a4aa98e97e4\0.0.0\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)
  0%|          | 0/1 [00:00<?, ?it/s]100%|??????????| 1/1 [00:00<?, ?it/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|????      | 1/3 [00:00<00:01,  1.68ba/s] 67%|???????   | 2/3 [00:01<00:00,  1.86ba/s]100%|??????????| 3/3 [00:01<00:00,  1.92ba/s]100%|??????????| 3/3 [00:01<00:00,  1.88ba/s]
***** Running Prediction *****
  Num examples = 3000
  Batch size = 8
  0%|          | 0/375 [00:00<?, ?it/s]  1%|          | 2/375 [00:21<1:06:38, 10.72s/it]  1%|          | 3/375 [00:42<1:34:16, 15.21s/it]  1%|          | 4/375 [01:04<1:47:46, 17.43s/it]Traceback (most recent call last):
  File "C:\Users\Phuong\PycharmProjects\tosdr\classifier\dl\bert.py", line 92, in <module>
    clf.predict('data/tosware_test.csv')
  File "C:\Users\Phuong\PycharmProjects\tosdr\classifier\dl\bert.py", line 68, in predict
    predictions = self.trainer.predict(dataset['train'])
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 2358, in predict
    output = eval_loop(
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 2458, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 2671, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 2043, in compute_loss
    outputs = model(**inputs)
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 1545, in forward
    outputs = self.bert(
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 996, in forward
    encoder_outputs = self.encoder(
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 585, in forward
    layer_outputs = layer_module(
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 330, in forward
    attention_probs = nn.functional.softmax(attention_scores, dim=-1)
  File "C:\Users\Phuong\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\functional.py", line 1818, in softmax
    ret = input.softmax(dim)
KeyboardInterrupt
  1%|          | 4/375 [01:12<1:52:10, 18.14s/it]
